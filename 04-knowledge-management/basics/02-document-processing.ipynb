{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æ¡£å¤„ç†ä¸åˆ†chunksç­–ç•¥\n",
    "> By SherryAGI | enterpriseæ–‡æ¡£çš„ç»“æ„åŒ–å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enterpriseå¸¸è§Document Type\n",
    "\n",
    "| ç±»å‹ | æ ¼å¼ | å¤„ç†éš¾åº¦ |\n",
    "|------|------|----------|\n",
    "| æ–‡æœ¬æ–‡æ¡£ | .txt, .md | â­ |\n",
    "| Office | .docx, .xlsx, .pptx | â­â­ |\n",
    "| PDF | .pdf | â­â­â­ |\n",
    "| ç½‘é¡µ | .html | â­â­ |\n",
    "| dataåº“ | SQL | â­â­â­ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "# pip install pypdf python-docx openpyxl beautifulsoup4\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print('ğŸš€ æ–‡æ¡£å¤„ç†module by SherryAGI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Loadingå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    \"\"\"å¤šæ ¼å¼Document Loadingå™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_txt(file_path):\n",
    "        \"\"\"åŠ è½½æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdf(file_path):\n",
    "        \"\"\"åŠ è½½ PDF æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            from pypdf import PdfReader\n",
    "            reader = PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except ImportError:\n",
    "            return \"è¯·å®‰è£… pypdf: pip install pypdf\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_docx(file_path):\n",
    "        \"\"\"åŠ è½½ Word æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            from docx import Document\n",
    "            doc = Document(file_path)\n",
    "            return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        except ImportError:\n",
    "            return \"è¯·å®‰è£… python-docx: pip install python-docx\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_xlsx(file_path):\n",
    "        \"\"\"åŠ è½½ Excel æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_excel(file_path)\n",
    "            return df.to_string()\n",
    "        except ImportError:\n",
    "            return \"è¯·å®‰è£… openpyxl: pip install openpyxl\"\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        \"\"\"è‡ªåŠ¨è¯†åˆ«æ ¼å¼å¹¶åŠ è½½\"\"\"\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        loaders = {\n",
    "            '.txt': cls.load_txt,\n",
    "            '.md': cls.load_txt,\n",
    "            '.pdf': cls.load_pdf,\n",
    "            '.docx': cls.load_docx,\n",
    "            '.xlsx': cls.load_xlsx,\n",
    "        }\n",
    "        loader = loaders.get(ext)\n",
    "        if loader:\n",
    "            return loader(file_path)\n",
    "        return f\"ä¸æ”¯æŒçš„æ ¼å¼: {ext}\"\n",
    "\n",
    "print('âœ… DocumentLoader defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆ†chunksç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": ,
   "outputs": [],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"Text Chunkingå™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_size(text, chunk_size=500, overlap=50):\n",
    "        \"\"\"Fixed-size chunking\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunks.append(text[start:end])\n",
    "            start = end - overlap\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def by_sentence(text, max_chunk_size=500):\n",
    "        \"\"\"æŒ‰å¥å­åˆ†chunks\"\"\"\n",
    "        import re\n",
    "        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\\s*', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
    "                current_chunk += sentence\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def by_paragraph(text):\n",
    "        \"\"\"by paragraphåˆ†chunks\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        return [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic(text, chunk_size=500):\n",
    "        \"\"\"è¯­ä¹‰åˆ†chunksï¼ˆåŸºäºæ ‡é¢˜/ç« èŠ‚ï¼‰\"\"\"\n",
    "        import re\n",
    "        # æŒ‰æ ‡é¢˜åˆ†å‰²\n",
    "        sections = re.split(r'\\n(?=#{1,3}\\s|\\d+\\.\\s)', text)\n",
    "        \n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            if len(section) <= chunk_size:\n",
    "                chunks.append(section.strip())\n",
    "            else:\n",
    "                # å¤§æ®µè½å†ç»†åˆ†\n",
    "                sub_chunks = TextChunker.fixed_size(section, chunk_size)\n",
    "                chunks.extend(sub_chunks)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print('âœ… TextChunker defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åˆ†chunksç­–ç•¥å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exampleæ–‡æ¡£\n",
    "sample_doc = \"\"\"# Employee Handbook\n",
    "\n",
    "## Chapter 1 Company Introduction\n",
    "\n",
    "Our company was founded in2020å¹´ï¼Œfocusing on AI technology R&Dã€‚Headquarters located inSwissZurichï¼Œwith over500employees worldwideã€‚Our mission is to useAItechnology to change the worldã€‚\n",
    "\n",
    "## Chapter 2 Onboarding Process\n",
    "\n",
    "New employees need to complete the following stepsï¼š\n",
    "1. Sign employment contract\n",
    "2. Submit ID documents\n",
    "3. Attend orientation training\n",
    "4. Receive office equipment\n",
    "\n",
    "å…¥èŒåŸ¹è®­ä¸ºæœŸä¸‰daysï¼Œincluding company cultureã€product introductionã€safety training, etc.ã€‚\n",
    "\n",
    "## Chapter 3 Compensation & Benefits\n",
    "\n",
    "Company offers competitive compensationï¼Œincludingï¼š\n",
    "- Base salary\n",
    "- Performance bonus\n",
    "- Year-end bonus\n",
    "- Stock options\n",
    "\n",
    "ç¦åˆ©includingï¼šsocial insuranceã€supplementary medical insuranceã€annual health checkã€å¸¦è–ªAnnual leaveç­‰ã€‚\n",
    "\"\"\"\n",
    "\n",
    "print('ğŸ“„ Original document length:', len(sample_doc), 'characters')\n",
    "print('\\n--- Chunking strategy comparison ---\\n')\n",
    "\n",
    "# å›ºå®šå¤§å°\n",
    "fixed_chunks = TextChunker.fixed_size(sample_doc, 200, 30)\n",
    "print(f'Fixed-size chunking: {len(fixed_chunks)} chunks')\n",
    "\n",
    "# by paragraph\n",
    "para_chunks = TextChunker.by_paragraph(sample_doc)\n",
    "print(f'by paragraphåˆ†chunks: {len(para_chunks)} chunks')\n",
    "\n",
    "# è¯­ä¹‰åˆ†chunks\n",
    "semantic_chunks = TextChunker.semantic(sample_doc, 300)\n",
    "print(f'è¯­ä¹‰åˆ†chunks: {len(semantic_chunks)} chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹è¯­ä¹‰åˆ†chunksResult\n",
    "print('ğŸ“ è¯­ä¹‰åˆ†chunksResult:\\n')\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f'--- chunks {i+1} ({len(chunk)} characters) ---')\n",
    "    print(chunk[:150] + '...' if len(chunk) > 150 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNYdataæå–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataExtractor:\n",
    "    \"\"\"CNYdataæå–å™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_path(file_path):\n",
    "        \"\"\"ä»æ–‡ä»¶pathæå–CNYdata\"\"\"\n",
    "        path = Path(file_path)\n",
    "        return {\n",
    "            \"filename\": path.name,\n",
    "            \"extension\": path.suffix,\n",
    "            \"directory\": str(path.parent),\n",
    "            \"size_bytes\": path.stat().st_size if path.exists() else 0\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_content(text):\n",
    "        \"\"\"ä»å†…å®¹æå–CNYdata\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # æå–æ ‡é¢˜\n",
    "        title_match = re.search(r'^#\\s+(.+)$', text, re.MULTILINE)\n",
    "        title = title_match.group(1) if title_match else None\n",
    "        \n",
    "        # æå–ç« èŠ‚\n",
    "        sections = re.findall(r'^##\\s+(.+)$', text, re.MULTILINE)\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"sections\": sections,\n",
    "            \"char_count\": len(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"line_count\": len(text.split('\\n'))\n",
    "        }\n",
    "\n",
    "# Test\n",
    "metadata = MetadataExtractor.extract_from_content(sample_doc)\n",
    "print('ğŸ“‹ æå–çš„CNYdata:')\n",
    "for k, v in metadata.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Completeå¤„ç†æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"æ–‡æ¡£å¤„ç†å™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=500, chunk_overlap=50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def process(self, file_path=None, text=None, metadata=None):\n",
    "        \"\"\"å¤„ç†æ–‡æ¡£ï¼Œè¿”å›å¸¦CNYdataçš„åˆ†chunks\"\"\"\n",
    "        \n",
    "        # 1. åŠ è½½æ–‡æ¡£\n",
    "        if file_path:\n",
    "            content = DocumentLoader.load(file_path)\n",
    "            file_metadata = MetadataExtractor.extract_from_path(file_path)\n",
    "        else:\n",
    "            content = text\n",
    "            file_metadata = {}\n",
    "        \n",
    "        # 2. æå–å†…å®¹CNYdata\n",
    "        content_metadata = MetadataExtractor.extract_from_content(content)\n",
    "        \n",
    "        # 3. åˆ†chunks\n",
    "        chunks = TextChunker.semantic(content, self.chunk_size)\n",
    "        \n",
    "        # 4. ç»„è£…Result\n",
    "        results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            results.append({\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    **file_metadata,\n",
    "                    **content_metadata,\n",
    "                    **(metadata or {}),\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ä½¿ç”¨Example\n",
    "processor = DocumentProcessor(chunk_size=300)\n",
    "processed = processor.process(text=sample_doc, metadata={\"source\": \"Employee Handbook\", \"version\": \"2026\"})\n",
    "\n",
    "print(f'âœ… Processing completeï¼Œå…± {len(processed)} chunks')\n",
    "print(f'\\nç¬¬ä¸€chunksçš„CNYdata:')\n",
    "for k, v in processed[0][\"metadata\"].items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "| Document Type | recommendedåˆ†chunksç­–ç•¥ | chunkså¤§å° |\n",
    "|----------|--------------|--------|\n",
    "| Technical docs | è¯­ä¹‰åˆ†chunksï¼ˆby sectionï¼‰ | 500-1000 |\n",
    "| Policy docs | by paragraph | 300-500 |\n",
    "| FAQ | æŒ‰é—®ç­”å¯¹ | 200-400 |\n",
    "| Code | by function/class/ç±» | 500-800 |\n",
    "| èŠdaysè®°å½• | by conversation turn | 300-500 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**By SherryAGI** | [DigitalTransformationAI](https://github.com/AIB612/DigitalTransformationAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
