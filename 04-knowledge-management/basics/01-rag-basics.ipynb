{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG åŸºç¡€æ¶æ„\n",
    "> By SherryAGI | Retrieval-Augmented Generation basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAGï¼Ÿ\n",
    "\n",
    "**Retrieval-Augmented Generation** = Retrievalå¢å¼ºGeneration\n",
    "\n",
    "```\n",
    "ç”¨æˆ·æé—® â†’ Retrievalç›¸å…³æ–‡æ¡£ â†’ ç»“åˆæ–‡æ¡£GenerationAnswer\n",
    "```\n",
    "\n",
    "### Why do we need RAGï¼Ÿ\n",
    "\n",
    "| Question | çº¯ LLM | RAG |\n",
    "|------|--------|-----|\n",
    "| knowledgeè¿‡æ—¶ | âŒ è®­ç»ƒdataæˆªæ­¢ | âœ… å®æ—¶Retrievalæœ€æ–°æ–‡æ¡£ |\n",
    "| ç§æœ‰data | âŒ æ— æ³•è®¿é—® | âœ… Connectionenterpriseknowledgeåº“ |\n",
    "| å¹»è§‰Question | âŒ å¯èƒ½ç¼–é€  | âœ… åŸºäºçœŸå®æ–‡æ¡£ |\n",
    "| Sourceè¿½æº¯ | âŒ æ— æ³•éªŒè¯ | âœ… å¼•ç”¨åŸæ–‡å‡ºå¤„ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "# pip install langchain langchain-openai chromadb\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "print('ğŸš€ RAG åŸºç¡€æ¶æ„ by SherryAGI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Core Process\n",
    "\n",
    "```\n",
    "1. Document Loading (Load)\n",
    "   â†“\n",
    "2. Text Chunking (Split)\n",
    "   â†“\n",
    "3. Vectorization (Embed)\n",
    "   â†“\n",
    "4. Storage (Store)\n",
    "   â†“\n",
    "5. Retrieval (Retrieve)\n",
    "   â†“\n",
    "6. Generation (Generate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exampleï¼šenterpriseknowledgeæ–‡æ¡£\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"Company reimbursement policyï¼š\n",
    "        1. Travel expensesï¼šeconomy class ticketsã€3-star hotels\n",
    "        2. Meal expensesï¼šdaily limit 100 CNY\n",
    "        3. Transportationï¼špublic transport or taxiï¼ˆreceipt requiredï¼‰\n",
    "        4. Approval processï¼š500CNYdirect reimbursementï¼Œ500CNYrequires manager approval\"\"\",\n",
    "        \"metadata\": {\"source\": \"HRæ”¿ç­–\", \"category\": \"æŠ¥é”€\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Leave policyï¼š\n",
    "        1. Annual leaveï¼šafter1years entitled to5daysï¼Œeach additional1year adds1daysï¼Œmaximum15days\n",
    "        2. Sick leaveï¼šrequires hospital certificateï¼Œannual cumulative maximum30days\n",
    "        3. Personal leaveï¼šrequires advance notice of3daysç”³è¯·ï¼Œannual cumulative maximum10days\n",
    "        4. approvalï¼š3daysä»¥å†…ä¸»ç®¡approvalï¼Œ3daysä»¥ä¸Šéœ€HRapproval\"\"\",\n",
    "        \"metadata\": {\"source\": \"HRæ”¿ç­–\", \"category\": \"è¯·å‡\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"IT Support processï¼š\n",
    "        1. Submit ticketï¼švia internal system\n",
    "        2. Response timeï¼šNormal issues24hoursï¼ŒUrgent issues4hours\n",
    "        3. Equipment requestï¼šéœ€éƒ¨é—¨ä¸»ç®¡approval\n",
    "        4. Password resetï¼šself-service or contact IT hotline 8888\"\"\",\n",
    "        \"metadata\": {\"source\": \"ITæ‰‹å†Œ\", \"category\": \"æ”¯æŒ\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f'Loaded {len(documents)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†chunksç­–ç•¥\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,      # æ¯chunksæœ€å¤§charactersæ•°\n",
    "    chunk_overlap=50,    # chunksä¹‹é—´é‡å charactersæ•°\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼Œ\", \" \"]\n",
    ")\n",
    "\n",
    "# åˆ†chunks\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    splits = text_splitter.split_text(doc[\"content\"])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"content\": split,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })\n",
    "\n",
    "print(f'After chunking: {len(chunks)} ä¸ªChunk')\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f'\\n--- Chunk {i+1} ---')\n",
    "    print(chunk[\"content\"][:100] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorization & Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ OpenAI Embeddingsï¼ˆéœ€è¦ API Keyï¼‰\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "# æˆ–ä½¿ç”¨æœ¬åœ°Modelsï¼ˆrecommendedenterpriseä½¿ç”¨ï¼‰\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "\n",
    "# Exampleï¼šæ¨¡æ‹ŸVectorization\n",
    "import numpy as np\n",
    "\n",
    "def mock_embed(text):\n",
    "    \"\"\"æ¨¡æ‹ŸVectorizationï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå® Embedding Modelsï¼‰\"\"\"\n",
    "    np.random.seed(hash(text) % 2**32)\n",
    "    return np.random.rand(384).tolist()\n",
    "\n",
    "# Vectorizationæ‰€æœ‰Chunk\n",
    "for chunk in chunks:\n",
    "    chunk[\"embedding\"] = mock_embed(chunk[\"content\"])\n",
    "\n",
    "print(f'Vector dimension: {len(chunks[0][\"embedding\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search(query, top_k=3):\n",
    "    \"\"\"è¯­ä¹‰æœç´¢\"\"\"\n",
    "    query_embedding = mock_embed(query)\n",
    "    \n",
    "    # è®¡ç®—Similarity\n",
    "    similarities = []\n",
    "    for chunk in chunks:\n",
    "        sim = cosine_similarity([query_embedding], [chunk[\"embedding\"]])[0][0]\n",
    "        similarities.append((sim, chunk))\n",
    "    \n",
    "    # æ’åºè¿”å› Top K\n",
    "    similarities.sort(key=lambda x: -x[0])\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# TestRetrieval\n",
    "query = \"What is the travel reimbursement policyï¼Ÿ\"\n",
    "results = search(query)\n",
    "\n",
    "print(f'ğŸ” Query: {query}\\n')\n",
    "for i, (score, chunk) in enumerate(results):\n",
    "    print(f'--- Result {i+1} (Similarity: {score:.3f}) ---')\n",
    "    print(f'Source: {chunk[\"metadata\"][\"source\"]}')\n",
    "    print(chunk[\"content\"][:150])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GenerationAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context_chunks):\n",
    "    \"\"\"åŸºäºRetrievalResultGenerationAnswerï¼ˆExampleï¼‰\"\"\"\n",
    "    \n",
    "    # æ„å»º Prompt\n",
    "    context = \"\\n\\n\".join([c[\"content\"] for _, c in context_chunks])\n",
    "    \n",
    "    prompt = f\"\"\"åŸºäºä»¥ä¸‹enterpriseknowledgeåº“å†…å®¹å›ç­”Questionã€‚å¦‚æœæ— æ³•ä»å†…å®¹ä¸­æ‰¾åˆ°Answerï¼Œè¯·è¯´æ˜ã€‚\n",
    "\n",
    "knowledgeåº“å†…å®¹ï¼š\n",
    "{context}\n",
    "\n",
    "Questionï¼š{query}\n",
    "\n",
    "Answerï¼š\"\"\"\n",
    "    \n",
    "    # å®é™…ä½¿ç”¨æ—¶è°ƒç”¨ LLM\n",
    "    # response = llm.invoke(prompt)\n",
    "    \n",
    "    print('ğŸ“ Generationçš„ Prompt:')\n",
    "    print(prompt)\n",
    "    print('\\nğŸ’¡ æç¤ºï¼šå°†æ­¤ Prompt å‘é€ç»™ LLM å³å¯è·å¾—Answer')\n",
    "\n",
    "generate_answer(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"ç®€å• RAG å®ç° by SherryAGI\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunks = []\n",
    "    \n",
    "    def add_document(self, content, metadata=None):\n",
    "        \"\"\"Add document\"\"\"\n",
    "        chunk = {\n",
    "            \"content\": content,\n",
    "            \"metadata\": metadata or {},\n",
    "            \"embedding\": mock_embed(content)\n",
    "        }\n",
    "        self.chunks.append(chunk)\n",
    "    \n",
    "    def query(self, question, top_k=3):\n",
    "        \"\"\"Query\"\"\"\n",
    "        results = search(question, top_k)\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"sources\": [c[\"metadata\"].get(\"source\", \"Unknown\") for _, c in results],\n",
    "            \"context\": [c[\"content\"] for _, c in results]\n",
    "        }\n",
    "\n",
    "# ä½¿ç”¨Example\n",
    "rag = SimpleRAG()\n",
    "for doc in documents:\n",
    "    rag.add_document(doc[\"content\"], doc[\"metadata\"])\n",
    "\n",
    "result = rag.query(\"Annual leaveæœ‰å¤šå°‘daysï¼Ÿ\")\n",
    "print(f'Question: {result[\"question\"]}')\n",
    "print(f'Source: {result[\"sources\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**By SherryAGI** | [DigitalTransformationAI](https://github.com/AIB612/DigitalTransformationAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
